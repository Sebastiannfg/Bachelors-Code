{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'functional' from 'torch.nn' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfacenet_pytorch\u001b[39;00m \u001b[39mimport\u001b[39;00m InceptionResnetV1, MTCNN\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sebastian\\Anaconda\\lib\\site-packages\\facenet_pytorch\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minception_resnet_v1\u001b[39;00m \u001b[39mimport\u001b[39;00m InceptionResnetV1\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmtcnn\u001b[39;00m \u001b[39mimport\u001b[39;00m MTCNN, PNet, RNet, ONet, prewhiten, fixed_image_standardization\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdetect_face\u001b[39;00m \u001b[39mimport\u001b[39;00m extract_face\n",
      "File \u001b[1;32mc:\\Users\\Sebastian\\Anaconda\\lib\\site-packages\\facenet_pytorch\\models\\inception_resnet_v1.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownload\u001b[39;00m \u001b[39mimport\u001b[39;00m download_url_to_file\n\u001b[0;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBasicConv2d\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'functional' from 'torch.nn' (unknown location)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary for video names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory you want to list files from\n",
    "directory_path = 'C:/DTU/3 - Tredje Aar/6 - Semester/Bachelor Projekt/Videos Clipped'\n",
    "\n",
    "# List all files in the directory\n",
    "families = os.listdir(directory_path)\n",
    "all_data = {}                                                           # A dictionary with each family member and the names of their videos\n",
    "\n",
    "for family in families:\n",
    "    persons = os.listdir(directory_path+\"/\"+str(family))\n",
    "    temp_list = []\n",
    "    for person in persons:\n",
    "        temp_list.append([person,os.listdir(directory_path+\"/\"+str(family)+\"/\"+str(person))])\n",
    "        \n",
    "    all_data[family] = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'40': [['1',\n",
       "   ['40-1-2022-1-1.mp4.mp4',\n",
       "    '40-1-2022-1-2.mp4.mp4',\n",
       "    '40-1-2022-1-3.mp4.mp4',\n",
       "    '40-1-2022-1-4.mp4.mp4',\n",
       "    '40-1-2022-1-5.mp4.mp4']],\n",
       "  ['2',\n",
       "   ['40-2-2022-1-1.mp4.mp4',\n",
       "    '40-2-2022-1-2.mp4.mp4',\n",
       "    '40-2-2022-1-3.mp4.mp4',\n",
       "    '40-2-2022-1-4.mp4.mp4',\n",
       "    '40-2-2022-1-5.mp4.mp4',\n",
       "    '40-2-2022-1-6.mp4.mp4',\n",
       "    '40-3-2022-1-7.mp4.mp4']],\n",
       "  ['3',\n",
       "   ['40-3-2022-1-1.mp4.mp4',\n",
       "    '40-3-2022-1-2.mp4.mp4',\n",
       "    '40-3-2022-1-3.mp4.mp4',\n",
       "    '40-3-2022-1-4.mp4.mp4',\n",
       "    '40-3-2022-1-5.mp4.mp4',\n",
       "    '40-3-2022-1-6.mp4.mp4',\n",
       "    '40-3-2022-1-7.mp4.mp4']],\n",
       "  ['4', ['40-4-2023-1-1.mp4.mp4']]],\n",
       " '41': [['1',\n",
       "   ['41-1-2020-1-1.mp4',\n",
       "    '41-1-2020-1-10.mp4',\n",
       "    '41-1-2020-1-2.mp4',\n",
       "    '41-1-2020-1-3.mp4',\n",
       "    '41-1-2020-1-4.mp4',\n",
       "    '41-1-2020-1-5.mp4',\n",
       "    '41-1-2020-1-6.mp4',\n",
       "    '41-1-2020-1-7.mp4',\n",
       "    '41-1-2020-1-8.mp4',\n",
       "    '41-1-2020-1-9.mp4']],\n",
       "  ['2',\n",
       "   ['41-2-2020-1-1.mp4',\n",
       "    '41-2-2020-1-10.mp4',\n",
       "    '41-2-2020-1-11.mp4',\n",
       "    '41-2-2020-1-12.mp4',\n",
       "    '41-2-2020-1-14.mp4',\n",
       "    '41-2-2020-1-15.mp4',\n",
       "    '41-2-2020-1-2.mp4',\n",
       "    '41-2-2020-1-3.mp4',\n",
       "    '41-2-2020-1-4.mp4',\n",
       "    '41-2-2020-1-5.mp4',\n",
       "    '41-2-2020-1-6.mp4',\n",
       "    '41-2-2020-1-7.mp4',\n",
       "    '41-2-2020-1-8.mp4',\n",
       "    '41-2-2020-1-9.mp4']],\n",
       "  ['3',\n",
       "   ['41-3-2020-1-1.mp4',\n",
       "    '41-3-2020-1-10.mp4',\n",
       "    '41-3-2020-1-11.mp4',\n",
       "    '41-3-2020-1-12.mp4',\n",
       "    '41-3-2020-1-2.mp4',\n",
       "    '41-3-2020-1-3.mp4',\n",
       "    '41-3-2020-1-4.mp4',\n",
       "    '41-3-2020-1-5.mp4',\n",
       "    '41-3-2020-1-6.mp4',\n",
       "    '41-3-2020-1-7.mp4',\n",
       "    '41-3-2020-1-8.mp4',\n",
       "    '41-3-2020-1-9.mp4',\n",
       "    '41-3-2020-2-1.mp4']]],\n",
       " '42': [['1',\n",
       "   ['42-1-2011-1-1.mp4',\n",
       "    '42-1-2011-1-2.mp4',\n",
       "    '42-1-2011-1-3.mp4',\n",
       "    '42-1-2011-1-4.mp4',\n",
       "    '42-1-2011-1-5.mp4',\n",
       "    '42-1-2011-1-6.mp4',\n",
       "    '42-1-2016-2-1.mp4',\n",
       "    '42-1-2016-2-2.mp4',\n",
       "    '42-1-2016-2-3.mp4']],\n",
       "  ['2', ['42-2-2011-1-1.mp4']]],\n",
       " '43': [['1',\n",
       "   ['43-1-2016-1-1.mp4',\n",
       "    '43-1-2016-1-2.mp4',\n",
       "    '43-1-2016-1-3.mp4',\n",
       "    '43-1-2016-1-4.mp4',\n",
       "    '43-1-2022-2-1.mp4',\n",
       "    '43-1-2022-2-2.mp4',\n",
       "    '43-1-2022-2-3.mp4',\n",
       "    '43-1-2022-2-4.mp4',\n",
       "    '43-1-2022-2-5.mp4']],\n",
       "  ['2',\n",
       "   ['43-2-2013-1-1.mp4',\n",
       "    '43-2-2013-1-2.mp4',\n",
       "    '43-2-2013-1-3.mp4',\n",
       "    '43-2-2013-1-4.mp4',\n",
       "    '43-2-2013-1-5.mp4',\n",
       "    '43-2-2017-3-1.mp4',\n",
       "    '43-2-2017-3-2.mp4',\n",
       "    '43-2-2017-3-3.mp4',\n",
       "    '43-2-2017-3-4.mp4',\n",
       "    '43-2-2019-2-1.mp4',\n",
       "    '43-2-2019-2-2.mp4',\n",
       "    '43-2-2019-2-3.mp4',\n",
       "    '43-2-2019-2-4.mp4',\n",
       "    '43-2-2019-2-5.mp4']],\n",
       "  ['3',\n",
       "   ['43-3-2022-1-1.mp4',\n",
       "    '43-3-2022-1-2.mp4',\n",
       "    '43-3-2022-1-3.mp4',\n",
       "    '43-3-2022-1-4.mp4']]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary of familial connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "relations_csv = pd.read_excel(\"relationsheet_kinship_detection (version 1).xlsb.xlsx\", sheet_name=\"Datasheet\")\n",
    "relations = [\"FS\", \"FD\", \"MS\", \"MD\", \"BB\", \"BS\", \"GM-GS\", \"GM-GD\", \"GF-GS\", \"GF-GD\"]\n",
    "relations_dict = {}                                                                                              # Dictionary containing all relationships (Including pairs)\n",
    "fam_pers_ids = list(relations_csv[\"Path\"])\n",
    "\n",
    "for i in range(len(fam_pers_ids)):\n",
    "    for fam in relations:\n",
    "        item = list(relations_csv[fam])[i]\n",
    "        if item == 0 or item == \"0\":\n",
    "            continue\n",
    "        if not isinstance(item, str):\n",
    "            continue\n",
    "        elif len(item) < 5:\n",
    "            relations_dict.setdefault(fam_pers_ids[i], []).append([item.replace(\" \", \"\"), fam])\n",
    "        else:\n",
    "            relations_dict.setdefault(fam_pers_ids[i], []).append([item.replace(\" \", \"\").split(\",\"), fam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relations_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m relations_dict\n",
      "\u001b[1;31mNameError\u001b[0m: name 'relations_dict' is not defined"
     ]
    }
   ],
   "source": [
    "relations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction and face alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'Videos Clipped/40/1/40-1-2022-1-1.mp4.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained FaceNet model for face detection and alignment\n",
    "base_model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "mtcnn = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "#Extract frame\n",
    "frame_number = 10\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)# Set the video file's position to the desired frame number\n",
    "ret, frame = cap.read()# Read the frame from the video file\n",
    "cap.release()# Close the video file\n",
    "\n",
    "#Convert to PIL.\n",
    "if ret:#ret is False if no frame present\n",
    "    # Convert the OpenCV frame (BGR) to a PIL image (RGB)\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "else:\n",
    "    print(\"Failed to read frame.\")\n",
    "\n",
    "#Align face image for better training\n",
    "aligned_image = mtcnn(image)\n",
    "#Create face embeddings using CNN model\n",
    "face_embedding = base_model(aligned_image.unsqueeze(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feedforward neural network using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Load the dataset\n",
    "dataframe = pd.read_csv(\"your_data.csv\")  # assuming your data is stored in a CSV file\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = np.array([list(map(float, x.split(\",\"))) for x in dataframe[\"minus_product\"]])\n",
    "y = np.array(dataframe[\"label\"]).reshape(-1, 1)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "# Convert the dataset to PyTorch tensors\n",
    "X_train, X_val = torch.from_numpy(X_train), torch.from_numpy(X_val)\n",
    "y_train, y_val = torch.from_numpy(y_train), torch.from_numpy(y_val)\n",
    "\n",
    "# Define the neural network and optimizer\n",
    "model = SimpleClassifier(input_size=2)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model on the training set and evaluate on the validation set\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    perm = torch.randperm(X_train.size(0))\n",
    "    X_train, y_train = X_train[perm], y_train[perm]\n",
    "    \n",
    "    # Train the model in batches\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        # Get a batch of data\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        # Zero the gradients and compute the forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch.float())\n",
    "        \n",
    "        # Compute the loss and gradients\n",
    "        loss = criterion(outputs, y_batch.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Evaluate the model on the validation set\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val.float())\n",
    "        preds = (outputs > 0.5).float()\n",
    "        accuracy = (preds == y_val).float().mean()\n",
    "        print(f\"Epoch {epoch+1}: Validation accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nelse:\\n    # Loop through the video frames\\n    while True:\\n        # Read the next frame\\n        ret, frame = video.read()\\n\\n        # Break the loop if we have reached the end of the video\\n        if not ret:\\n            break\\n\\n        # Process the frame using a convolutional neural network\\n        # Your CNN code here\\n\\n        # Show the frame\\n        cv2.imshow('Video Frame', frame)\\n\\n        # Wait for a key press and break the loop if the 'q' key is pressed\\n        if cv2.waitKey(1) & 0xFF == ord('q'):\\n            break\\n\\n    # Release the video file\\n    video.release()\\n\\n    # Close any OpenCV windows\\n    cv2.destroyAllWindows()\\n\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace this with one of the video file paths from your list\n",
    "video_path = 'Videos Clipped/40/1/40-1-2022-1-1.mp4.mp4'\n",
    "\n",
    "# Read the video file\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video file was opened successfully\n",
    "if not video.isOpened():\n",
    "    print(f\"Error: Could not open video file {video_path}\")\n",
    "else:\n",
    "    ret,frame = video.read()\n",
    "    print(ret)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
