{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "import networkx as nx\n",
    "import netwulf\n",
    "import urllib.request"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation functions defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_fam(fam): #possibly unnecessary\n",
    "    \"\"\"function for flipping family string symbol\n",
    "\n",
    "    Args:\n",
    "        fam (str): a string of two or five letters with family relation : FS, MS, GM-GS etc.\n",
    "\n",
    "    Returns:\n",
    "        str : the inverted version of fam : FS to SF, GM-GS to GS-GM\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if not isinstance(fam,str):\n",
    "        print(\"Converted to string\")\n",
    "        fam = str(fam)\n",
    "    if len(fam) == 2:\n",
    "        flipped_family = fam[1] + fam[0] \n",
    "        return flipped_family\n",
    "    else:\n",
    "        fam1,fam2 = fam.split(\"-\")\n",
    "        flipped_family == str(fam2) + \"-\" + str(fam1)\n",
    "        return(flipped_family)\n",
    "\n",
    "def ret_edges_fam(fam):\n",
    "    \"\"\"Returns all edges of type fam\n",
    "\n",
    "    Args:\n",
    "        fam (str) : a string of two or five letters with family relation : FS, MS, GM-GS etc.\n",
    "        \n",
    "    Returns:\n",
    "        list[edges] : a list of edges of type fam\n",
    "    \"\"\"\n",
    "    if isinstance(fam,str):    \n",
    "        return [(u, v) for u, v, attr in G.edges(data=True) if attr['label']==fam]\n",
    "    else:\n",
    "        print(\"Converted to string\")\n",
    "        return [(u, v) for u, v, attr in G.edges(data=True) if attr['label']==str(fam)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary for video names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory you want to list files from\n",
    "directory_path = 'C:/DTU/3 - Tredje Aar/6 - Semester/Bachelor Projekt/Bachelors code/Videos Clipped'\n",
    "\n",
    "# List all files in the directory\n",
    "families = os.listdir(directory_path)\n",
    "all_data = {}                                                           # A dictionary with each family member and the names of their videos\n",
    "\n",
    "for family in families:\n",
    "    persons = os.listdir(directory_path+\"/\"+str(family))\n",
    "    temp_list = []\n",
    "    for person in persons:\n",
    "        temp_list.append([person,os.listdir(directory_path+\"/\"+str(family)+\"/\"+str(person))])\n",
    "        \n",
    "    all_data[family] = temp_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary of familial connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "relations_path = \"Metadata - Gathered data/relationsheet_kinship_detection (version 1).xlsb.xlsx\"\n",
    "relations_csv = pd.read_excel(relations_path, sheet_name=\"Datasheet\")\n",
    "relations = [\"FS\", \"FD\", \"MS\", \"MD\", \"BB\", \"BS\", \"GM-GS\", \"GM-GD\", \"GF-GS\", \"GF-GD\"]\n",
    "relations_dict = {}                                                                                              # Dictionary containing all relationships (Including pairs)\n",
    "fam_pers_ids = list(relations_csv[\"Path\"])\n",
    "\n",
    "for i in range(len(fam_pers_ids)):\n",
    "    for fam in relations:\n",
    "        item = list(relations_csv[fam])[i]\n",
    "        if item == 0 or item == \"0\":\n",
    "            continue\n",
    "        if not isinstance(item, str):\n",
    "            continue\n",
    "        elif len(item) < 5:\n",
    "            relations_dict.setdefault(fam_pers_ids[i], []).append([item.replace(\" \", \"\"), fam])\n",
    "        else:\n",
    "            relations_dict.setdefault(fam_pers_ids[i], []).append([item.replace(\" \", \"\").split(\",\"), fam])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a graph using the links\n",
    "This is done as it is by far the easiest way to manipulate this set of connections, especially when only considering kin of any kind, rather than kin of a specific nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Create an empty directed graph\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m G \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mDiGraph()\n\u001b[0;32m      4\u001b[0m \u001b[39m#Creating nodes (family members)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m person \u001b[39min\u001b[39;00m relations_dict\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an empty directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "#Creating nodes (family members)\n",
    "for person in relations_dict.keys():\n",
    "    G.add_node(person)\n",
    "    for relation in relations_dict[person]:\n",
    "        fam = person.split(\"/\")[0]\n",
    "        if isinstance(relation[0], list):\n",
    "            for relation_more in relation[0]:\n",
    "                G.add_edge(fam+\"/\"+relation_more[0],fam+\"/\"+relation_more[-1], label=relation[-1])\n",
    "        else:\n",
    "            G.add_edge(fam+\"/\"+relation[0][0],fam+\"/\"+relation[0][-1], label=relation[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netwulf.visualize(G) # Here you can visualize the graph if desired"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facenet Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained FaceNet model\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-&export=download'\n",
    "model_filename = '20180402-114759.pb'\n",
    "\n",
    "if not os.path.exists(model_filename):\n",
    "    urllib.request.urlretrieve(url, model_filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction and face alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'Videos Clipped/40/1/40-1-2022-1-1.mp4.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained FaceNet model for face detection and alignment\n",
    "base_model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "#Extract frame\n",
    "frame_number = 10\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)# Set the video file's position to the desired frame number\n",
    "ret, frame = cap.read()# Read the frame from the video file\n",
    "cap.release()# Close the video file\n",
    "\n",
    "#Convert to PIL.\n",
    "if ret:#ret is False if no frame present\n",
    "    # Convert the OpenCV frame (BGR) to a PIL image (RGB)\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "else:\n",
    "    print(\"Failed to read frame.\")\n",
    "\n",
    "#Align face image for better training\n",
    "aligned_image = mtcnn(image)\n",
    "#Create face embeddings using CNN model\n",
    "face_embedding = base_model(aligned_image.unsqueeze(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feedforward neural network using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Load the dataset\n",
    "dataframe = pd.read_csv(\"your_data.csv\")  # assuming your data is stored in a CSV file\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = np.array([list(map(float, x.split(\",\"))) for x in dataframe[\"minus_product\"]])\n",
    "y = np.array(dataframe[\"label\"]).reshape(-1, 1)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "# Convert the dataset to PyTorch tensors\n",
    "X_train, X_val = torch.from_numpy(X_train), torch.from_numpy(X_val)\n",
    "y_train, y_val = torch.from_numpy(y_train), torch.from_numpy(y_val)\n",
    "\n",
    "# Define the neural network and optimizer\n",
    "model = SimpleClassifier(input_size=2)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model on the training set and evaluate on the validation set\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    perm = torch.randperm(X_train.size(0))\n",
    "    X_train, y_train = X_train[perm], y_train[perm]\n",
    "    \n",
    "    # Train the model in batches\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        # Get a batch of data\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        # Zero the gradients and compute the forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch.float())\n",
    "        \n",
    "        # Compute the loss and gradients\n",
    "        loss = criterion(outputs, y_batch.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Evaluate the model on the validation set\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val.float())\n",
    "        preds = (outputs > 0.5).float()\n",
    "        accuracy = (preds == y_val).float().mean()\n",
    "        print(f\"Epoch {epoch+1}: Validation accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nelse:\\n    # Loop through the video frames\\n    while True:\\n        # Read the next frame\\n        ret, frame = video.read()\\n\\n        # Break the loop if we have reached the end of the video\\n        if not ret:\\n            break\\n\\n        # Process the frame using a convolutional neural network\\n        # Your CNN code here\\n\\n        # Show the frame\\n        cv2.imshow('Video Frame', frame)\\n\\n        # Wait for a key press and break the loop if the 'q' key is pressed\\n        if cv2.waitKey(1) & 0xFF == ord('q'):\\n            break\\n\\n    # Release the video file\\n    video.release()\\n\\n    # Close any OpenCV windows\\n    cv2.destroyAllWindows()\\n\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace this with one of the video file paths from your list\n",
    "video_path = 'Videos Clipped/40/1/40-1-2022-1-1.mp4.mp4'\n",
    "\n",
    "# Read the video file\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video file was opened successfully\n",
    "if not video.isOpened():\n",
    "    print(f\"Error: Could not open video file {video_path}\")\n",
    "else:\n",
    "    ret,frame = video.read()\n",
    "    print(ret)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
